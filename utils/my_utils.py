import numpy as np
import os
import logging
import json
import platform
import torch
import monai
import numpy
from tqdm import tqdm
import traceback
from typing import Dict, List, Tuple, Any
# í•™ìŠµ ì¬ê°œë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í•¨ìˆ˜
def acc_save_checkpoint(accelerator, model, optimizer, global_step, dice_val_best, global_step_best, epoch_loss_values, metric_values, output_dir):
    if accelerator.is_main_process:
        # ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ìƒíƒœëŠ” acceleratorë¥¼ ì‚¬ìš©í•´ ì €ì¥
        accelerator.save_state(output_dir)
        
        # ê¸°íƒ€ ë³€ìˆ˜ë“¤ì€ npz íŒŒì¼ë¡œ ì €ì¥
        np.savez(
            os.path.join(output_dir, "additional_state.npz"),
            global_step=global_step,
            dice_val_best=dice_val_best,
            global_step_best=global_step_best,
            epoch_loss_values=np.array(epoch_loss_values),
            metric_values=np.array(metric_values)
        )
        print(f"Checkpoint saved at step {global_step}")

# ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def acc_load_checkpoint(accelerator, model, optimizer, output_dir):
    # ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ìƒíƒœëŠ” acceleratorë¥¼ ì‚¬ìš©í•´ ë¡œë“œ
    accelerator.load_state(output_dir)
    
    # ê¸°íƒ€ ë³€ìˆ˜ë“¤ì€ npz íŒŒì¼ë¡œ ë¡œë“œ
    additional_state = np.load(os.path.join(output_dir, "additional_state.npz"))
    global_step = int(additional_state["global_step"])
    dice_val_best = float(additional_state["dice_val_best"])
    global_step_best = int(additional_state["global_step_best"])
    epoch_loss_values = additional_state["epoch_loss_values"].tolist()
    metric_values = additional_state["metric_values"].tolist()
    
    print(f"Checkpoint loaded from step {global_step}")
    return global_step, dice_val_best, global_step_best, epoch_loss_values, metric_values

# # TqdmLoggingHandler ì •ì˜
# class TqdmLoggingHandler(logging.Handler):
#     def __init__(self, level=logging.NOTSET):
#         super().__init__(level)

#     def emit(self, record):
#         try:
#             msg = self.format(record)
#             tqdm.write(msg)
#             self.flush()
#         except Exception:
#             self.handleError(record)


# def setup_logging(log_dir):
#     # ë¡œê·¸ í¬ë§· ì„¤ì •
#     log_format = "%(asctime)s - %(levelname)s - %(message)s"
    
#     # ë¡œê·¸ íŒŒì¼ ì €ì¥ ê²½ë¡œë¥¼ log_dirë¡œ ì§€ì •
#     log_file_path = os.path.join(log_dir, "training.log")
    
#     # Root logger ê°€ì ¸ì˜¤ê¸°
#     logger = logging.getLogger()
#     logger.setLevel(logging.INFO)

#     # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
#     if logger.hasHandlers():
#         logger.handlers.clear()

#     # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
#     file_handler = logging.FileHandler(log_file_path)
#     file_handler.setLevel(logging.INFO)
#     file_handler.setFormatter(logging.Formatter(log_format))

#     # tqdm í•¸ë“¤ëŸ¬ ì¶”ê°€
#     tqdm_handler = TqdmLoggingHandler()
#     tqdm_handler.setLevel(logging.INFO)
#     tqdm_handler.setFormatter(logging.Formatter(log_format))

#     # ë¡œê±°ì— í•¸ë“¤ëŸ¬ ì¶”ê°€
#     logger.addHandler(file_handler)
#     logger.addHandler(tqdm_handler)

#     logging.info("Logging is set up. Log file: %s", log_file_path)
    


def sample_pixels_randomly(X, y, num_sampled_pixels=8192):
    """
    BCL ì›ë³¸ ë°©ì‹ì²˜ëŸ¼ ë°°ì¹˜ í¬ê¸°ë§Œ ìœ ì§€í•˜ë©´ì„œ í”½ì…€ì„ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•˜ëŠ” í•¨ìˆ˜
    - X: [Batch, C, D, H, W]  (Feature Map)
    - y: [Batch, D, H, W]  (Label Map)
    - num_sampled_pixels: ì´ ìƒ˜í”Œë§í•  í”½ì…€ ìˆ˜
    """
    device = X.device
    batch_size, num_channels, D, H, W = X.shape  # ì…ë ¥ feature map í¬ê¸°

    # ì „ì²´ í”½ì…€ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
    all_pixels = (y >= 0).nonzero(as_tuple=True)  # ëª¨ë“  í”½ì…€ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°

    # ëœë¤í•˜ê²Œ num_sampled_pixels ê°œìˆ˜ë§Œí¼ ìƒ˜í”Œë§
    sampled_indices = torch.randint(0, len(all_pixels[0]), (num_sampled_pixels,))
    sampled_indices = [idx[sampled_indices] for idx in all_pixels]

    # ìƒ˜í”Œë§ëœ ìœ„ì¹˜ì—ì„œ feature ê°€ì ¸ì˜¤ê¸°
    X_samples = X[sampled_indices[0], :, sampled_indices[1], sampled_indices[2], sampled_indices[3]]
    y_samples = y[sampled_indices[0], sampled_indices[1], sampled_indices[2], sampled_indices[3]]

    return X_samples, y_samples



from typing import Dict, List, Union
import yaml


### 1. Config & Seed
# def load_config(config_path: str) -> Dict:
#     with open(config_path, 'r') as file:
#         return yaml.safe_load(file)

def safe_eval(val: str):
    lowered = val.lower()
    if lowered == "true":
        return True
    elif lowered == "false":
        return False

    try:
        return eval(val, {"__builtins__": None}, {})
    except:
        return val

def apply_overrides(config: dict, overrides: List[str]) -> dict:
    """
    override = ["train_params.batch_size=4", "model_params.img_size=[128,128,128]"]
    """
    for item in overrides:
        if '=' not in item:
            raise ValueError(f"Invalid override format: {item}")
        key_str, value_str = item.split("=", 1)
        keys = key_str.split(".")

        d = config
        for k in keys[:-1]:
            if k not in d:
                raise KeyError(f"Key '{k}' not found in config")
            d = d[k]

        d[keys[-1]] = safe_eval(value_str)
    return config

def load_config(path: str, overrides: List[str] = None) -> dict:
    """
    - path: path to YAML config
    - overrides: optional list of "key=value" strings
    """
    with open(path, "r") as f:
        config = yaml.safe_load(f)

    if overrides:
        config = apply_overrides(config, overrides)

    return config

import socket

def get_primary_ip():
    """ì™¸ë¶€ ë¼ìš°íŒ…ì„ ê¸°ì¤€ìœ¼ë¡œ ì„œë²„ê°€ ì‚¬ìš©í•˜ëŠ” IP"""
    try:
        # ì„ì˜ì˜ ì™¸ë¶€ ì£¼ì†Œì™€ ì†Œì¼“ ì—°ê²°ì„ ì‹œë„í•´ ì„œë²„ì˜ ì‹¤ì œ IP ì¶”ë¡ 
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))  # ì‹¤ì œë¡œ ì—°ê²°ë˜ì§„ ì•ŠìŒ
        ip = s.getsockname()[0]
        # ipëŠ” 115.145.145.167 ì´ëŸ°í˜•íƒœ
        s.close()
        return ip[-3:]
    except Exception:
        return "000"

def resolve_data_path(config: dict,path: str) -> str:
    if "<data_root>" in path:
        return path.replace("<data_root>", config(f"{get_primary_ip()}.data_root"))
    return path


import glob

def prune_checkpoints(output_dir, saved_checkpoints, top_k: int = 3):
    """output_dir ë‚´ checkpoint_epoch_*.pth ì¤‘
       saved_checkpointsì— ê¸°ë¡ëœ top_k epochë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ."""
    pattern = os.path.join(output_dir, "checkpoint_epoch_*.pth")
    files = glob.glob(pattern)
    # ì‚­ì œ ëŒ€ìƒì´ 10ê°œ ì´í•˜ì´ë©´ ë¬´ì‹œ
    if len(files) <= 5:
        return
    # íŒŒì¼ëª…ì—ì„œ epoch ë²ˆí˜¸ ì¶”ì¶œ
    epochs = []
    for f in files:
        try:
            epochs.append(int(os.path.basename(f).split("_")[-1].split(".pth")[0]))
        except ValueError:
            continue
    # (epoch, metric, filepath) ë¦¬ìŠ¤íŠ¸
    trio = [(e, saved_checkpoints.get(e, -1.0), os.path.join(output_dir, f"checkpoint_epoch_{e}.pth"))
            for e in epochs]
    # Dice ë‚´ë¦¼ì°¨ìˆœ ìƒìœ„ Kê°œ ì„ ì •
    keep_files = {
        filepath
        for _, _, filepath in sorted(trio, key=lambda x: x[1], reverse=True)[:top_k]
    }
    # ê·¸ ì™¸ íŒŒì¼ ì‚­ì œ ì‹œë„ (ì‹¤íŒ¨í•´ë„ ê²½ê³ ë§Œ)
    for _, _, filepath in trio:
        if filepath not in keep_files:
            try:
                os.remove(filepath)
                # logging.info(f"Pruned checkpoint {os.path.basename(filepath)}")
            except Exception as e:
                logging.warning(f"Failed to remove {filepath}: {e}")

from monai.transforms import RandomizableTransform, MapTransform
from monai.config import KeysCollection
from monai.data.meta_obj import get_track_meta
from monai.utils.type_conversion import convert_to_tensor
import numpy as np
from typing import Optional, Mapping, Hashable
from monai.data.meta_tensor import MetaTensor
from monai.transforms.transform import Randomizable

class RandInvertIntensityd(RandomizableTransform, MapTransform):
    """
    Dictionary-based random intensity inversion.

    Invert image intensity as: new_val = 1.0 - old_val
    Only applies to values assumed normalized in [0, 1].

    Args:
        keys: input keys to apply the transform.
        prob: probability of applying the transform.
        allow_missing_keys: don't raise exception if key is missing.
    """

    def __init__(self, keys: KeysCollection, prob: float = 0.1, allow_missing_keys: bool = False):
        MapTransform.__init__(self, keys, allow_missing_keys)
        RandomizableTransform.__init__(self, prob)

    def set_random_state(
        self, seed: Optional[int] = None, state: Optional[np.random.RandomState] = None
    ):
        super().set_random_state(seed, state)
        return self

    def __call__(self, data: Mapping[Hashable, np.ndarray]):
        d = dict(data)
        self.randomize(None)
        for key in self.key_iterator(d):
            # Always convert to tensor to maintain consistency, regardless of _do_transform
            d[key] = convert_to_tensor(d[key], track_meta=get_track_meta())
            if self._do_transform:
                d[key] = 1.0 - d[key]
        return d


if __name__ == "__main__":
    import argparse
    import pprint
    parser = argparse.ArgumentParser(description="Debug config loading")

    parser.add_argument(
        '--config', type=str,
        default="your_config.yaml",   # ì‹¤ì œ config ê²½ë¡œë¡œ ë°”ê¿”ì£¼ì„¸ìš”
        help="Path to the YAML config file"
    )

    parser.add_argument(
        '--override', nargs='*', default=[],
        help="Override config parameters, e.g., train_params.batch_size=4"
    )

    args = parser.parse_args()

    config = load_config(args.config, overrides=args.override)

    print("\nğŸ”§ Final Config (with overrides if given):")
    pprint.pprint(config)
